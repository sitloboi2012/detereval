# detereval
> Evaluate your RAG more determine ðŸ“ with __detereval__

Deterministic Evaluation aka __detereval__ is a framework aims to provide an interface where you can evaluate your Retrieval Augmented Generation (RAG) more determine by using pure NLP methodology. We might also looking forward to expand into CV and Multi-Modal aspect as well.

## Why __detereval__ ?

We understand that evaluate your RAG might be painful and difficult.  Current existing methods provide with more non-deterministic method by using LLM-as-a-Judge under the hood, making the process of evaluating a RAG more complex, low reproducibility with high bias and LLM-reliance.

> ðŸ¤« Chance that if you are using GPT-3.5-Turbo to evaluate your RAG, your result might be _"a bit"_ differences compare to the result from better the model like GPT-4-Turbo or Llama-3-70b - which required heavy machine ðŸ˜¥

At __detereval__, we want to provide you with a more robust and determine method helps you to analyse your RAG more effectively, increase the transparency and explainability in your RAG pipeline.
This way you improve your RAG performance by adjusting the correct components that required improvement.


## Categories

There are 4 main categories with each category covers the Question measurement aspect, Answer measurement aspect and Context measurement aspect

![image](detereval_img.png)

This work built upon previous foundation that has been layout by other method framework like RAGAS, RGB, TruLens. But we aim for more determine methods.

## Installation

__TBA__

## Quickstart

__TBA__